# Flan-T5 Configuration
model_name: "google/flan-t5-large"
model_type: "flan-t5"
architecture: "encoder-decoder"

# Model details
hidden_size: 1024
num_attention_heads: 16
num_hidden_layers: 24
max_length: 512

# LoRA Configuration
lora:
  enabled: true
  r: 8
  lora_alpha: 32
  lora_dropout: 0.1
  target_modules: ["q", "v"]

# Training
learning_rate: 3e-4
batch_size: 8
num_epochs: 10
gradient_accumulation_steps: 4
generation_max_length: 128

# Inference
do_sample: false
num_beams: 1



# name: "flant5"

# # Nom exact du modèle HuggingFace chargé
# hf_model: "google/flan-t5-small"
# # hf_model: "google/flan-t5-large"

# # ------------------------------
# # Précision & Quantification
# # ------------------------------
# dtype: "fp16"        # si pas de 4bit => torch_dtype=float16
# load_in_4bit: true   # active QLoRA automatiquement

# quantization_config:
#   bnb_4bit_compute_dtype: "float16"
#   bnb_4bit_quant_type: "nf4"
#   bnb_4bit_use_double_quant: true

# # ------------------------------
# # Paramètres d’entraînement
# # ------------------------------
# learning_rate: 2e-4
# batch_size: 4
# epochs: 3

# # Longueurs utilisées dans preprocess()
# max_input_length: 512
# max_target_length: 64

# # ------------------------------
# # Mapping des labels (nécessaire dans preprocess)
# # ------------------------------
# label_map:
#   "0": "entailment"
#   "1": "neutral"
#   "2": "contradiction"

# # ------------------------------
# # QLoRA
# # ------------------------------
# qlora:
#   r: 16
#   alpha: 32
#   dropout: 0.05
#   target_modules:
#     - "q"
#     - "k"
#     - "v"
#     - "o"

# # ------------------------------
# # Optionnel - HuggingFace Hub
# # ------------------------------
# push_to_hub: true
# hf_repo: "CHARLESL16/Flant5"
# hf_token: "your_huggingface_token_here "
